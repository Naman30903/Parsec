# Handler â†’ Producer Pipeline Integration

## âœ… Deliverable Complete

The end-to-end ingest pipeline is now fully functional with:
- âœ… Handler â†’ Producer integration
- âœ… Partial success handling for batch errors
- âœ… Async worker pool for message publishing
- âœ… Graceful shutdown with flush guarantees
- âœ… Complete error handling and retry logic

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        HTTP Request                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Ingest Handler      â”‚
                  â”‚  - Parse JSON        â”‚
                  â”‚  - Validate          â”‚
                  â”‚  - Normalize         â”‚
                  â”‚  - Create Envelope   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Envelope Channel    â”‚
                  â”‚  (Buffered: 1000)    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   â”‚                   â”‚
         â–¼                   â–¼                   â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Worker 1â”‚         â”‚ Worker 2â”‚   ...   â”‚ Worker Nâ”‚
   â”‚         â”‚         â”‚         â”‚         â”‚         â”‚
   â”‚ Batch   â”‚         â”‚ Batch   â”‚         â”‚ Batch   â”‚
   â”‚ Events  â”‚         â”‚ Events  â”‚         â”‚ Events  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Kafka Producer     â”‚
                 â”‚   - Pool (4 conns)   â”‚
                 â”‚   - Retry logic      â”‚
                 â”‚   - Compression      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Kafka Broker â”‚
                    â”‚  Topic: logs  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Components

### 1. **Ingest Handler** (`internal/handlers/ingest.go`)

**Responsibilities:**
- Parse JSON (single event, `{event: ...}`, `{events: []}`, or array)
- Validate each event (required fields, types, limits)
- Normalize fields (lowercase source, trim message, parse timestamp)
- Create envelope with metadata
- Push to channel (non-blocking with overflow protection)

**Error Handling:**
- Validation errors â†’ immediate rejection with details
- Channel full â†’ reject with "queue full" error
- Partial batch success â†’ returns both accepted & rejected counts

**Example Response:**
```json
{
  "success": true,
  "accepted": 8,
  "rejected": 2,
  "errors": [
    {
      "index": 3,
      "event_id": "evt-004",
      "error": "log event ID cannot be empty"
    },
    {
      "index": 7,
      "event_id": "evt-008",
      "error": "invalid severity level"
    }
  ]
}
```

### 2. **Worker Pool** (`internal/worker/worker.go`)

**Configuration:**
- Number of workers: 4 (default, configurable)
- Batch size: 100 events
- Batch timeout: 100ms
- Graceful shutdown with flush

**Features:**
- Concurrent workers consume from envelope channel
- Automatic batching for efficiency
- Timeout-based flushing (prevents starvation)
- Graceful shutdown flushes remaining events
- Fallback to individual publish on batch failure

**Processing Logic:**
```
Worker Loop:
  1. Read envelope from channel
  2. Add to batch
  3. If batch full OR timeout â†’ publish batch
  4. On error â†’ retry with exponential backoff
  5. On persistent error â†’ try individual publishes
  6. Update metrics
```

### 3. **Kafka Producer** (`internal/kafka/producer.go`)

**Features:**
- Connection pool (4 writers by default)
- Exponential backoff retry (3 attempts)
- Snappy compression
- Partition by tenant ID (ordering guarantee)
- Health check capability
- Metrics tracking

**Configuration:**
```go
Producer Config:
  - BatchSize: 100
  - BatchTimeout: 100ms
  - MaxRetries: 3
  - RetryBackoff: 100ms
  - RequiredAcks: -1 (wait for all replicas)
  - Compression: snappy
  - WriteTimeout: 10s
  - PoolSize: 4
```

### 4. **Processor** (`internal/processor/processor.go`)

**Orchestrates:**
- HTTP server lifecycle (port 8080)
- Kafka producer initialization
- Worker pool management
- Graceful shutdown sequence
- Statistics reporting (every 30s)

**Endpoints:**
- `POST /ingest` - Ingest log events
- `GET /health` - Health check (includes Kafka connectivity)
- `GET /stats` - Pipeline statistics

---

## Graceful Shutdown Sequence

When receiving SIGINT/SIGTERM:

```
1. Stop HTTP Server (10s timeout)
   â†“ No new requests accepted
   
2. Close Envelope Channel
   â†“ Signal workers: no more input
   
3. Stop Worker Pool (15s timeout)
   â†“ Workers flush remaining batches
   
4. Close Kafka Producer
   â†“ Flush any buffered messages
   
5. Wait for Goroutines
   â†“ Clean shutdown complete
```

**Guarantees:**
- No event loss during shutdown
- In-flight events are published
- Batches are flushed
- Connections closed cleanly

---

## Error Handling Strategy

### Validation Errors (400-level)
```
Handler validates â†’ Reject immediately â†’ Return to client
No enqueuing, no resource waste
```

### Publishing Errors (500-level)
```
1. Initial publish fails
2. Exponential backoff retry (3x)
3. If batch fails â†’ try individual publishes
4. Log failures for monitoring
5. Update failure metrics
```

### Backpressure
```
Channel buffer full â†’ Non-blocking send fails â†’ Return error
Client sees: "internal queue full, try again later"
```

---

## Testing

### Unit Tests âœ…
```bash
make test
```

Tests cover:
- âœ… Handler: single events, batches, validation
- âœ… Worker pool: batching, timeout flush, graceful shutdown
- âœ… Kafka producer: publish, batch, retries, health check
- âœ… Models: validation, normalization, timestamp parsing
- âœ… Processor: lifecycle, graceful shutdown

### Integration Test âœ…
```bash
make test-integration
```

Tests end-to-end:
- HTTP server startup
- Event ingestion (single + batch)
- Stats reporting
- Graceful shutdown

### Manual Testing
```bash
# Start infrastructure
make up

# Run processor
./bin/processor

# In another terminal, run tests
./test_pipeline.sh
```

---

## Performance Characteristics

### Throughput
- **Target:** 10,000+ events/sec
- **Bottleneck:** Kafka network latency
- **Optimization:** Batching (100 events/batch)

### Latency
- **P50:** < 50ms (validation + enqueue)
- **P99:** < 200ms (includes retry)
- **Timeout:** 10s (write timeout)

### Resource Usage
- **Memory:** Envelope channel buffer (1000 envelopes)
- **Goroutines:** 4 workers + HTTP handlers + stats reporter
- **Connections:** 4 Kafka connections (pooled)

---

## Configuration

### Environment Variables
```bash
# Kafka Settings
export KAFKA_BROKERS=localhost:9092
export KAFKA_TOPIC=log-events
export KAFKA_BATCH_SIZE=100
export KAFKA_BATCH_TIMEOUT_MS=100
export KAFKA_MAX_RETRIES=3
export KAFKA_POOL_SIZE=4
export KAFKA_COMPRESSION=snappy
```

### Tuning Guidelines

**High Throughput:**
- â†‘ `KAFKA_POOL_SIZE` (more workers)
- â†‘ `KAFKA_BATCH_SIZE` (larger batches)
- â†‘ Envelope channel buffer

**Low Latency:**
- â†“ `KAFKA_BATCH_TIMEOUT_MS` (faster flush)
- â†“ `KAFKA_BATCH_SIZE` (smaller batches)
- Use `compression=none`

**High Reliability:**
- â†‘ `KAFKA_MAX_RETRIES` (more retries)
- Set `RequiredAcks=-1` (all replicas)
- â†‘ `WriteTimeout`

---

## Monitoring & Observability

### Logs
- Startup/shutdown events
- HTTP request/response
- Worker pool activity
- Publish success/failures
- Periodic statistics (every 30s)

### Metrics (via `/stats`)
```json
{
  "worker": {
    "processed": 10000,
    "failed": 5
  },
  "producer": {
    "messages_sent": 9995,
    "messages_failed": 5,
    "bytes_written": 5242880
  },
  "channel": {
    "buffered": 42,
    "capacity": 1000
  }
}
```

### Health Check
```bash
curl http://localhost:8080/health
```

Returns:
- âœ… `200 OK` - System healthy
- âŒ `503 Service Unavailable` - Kafka unreachable

---

## What's Next?

1. **Storage Layer** - Implement ClickHouse/PostgreSQL writers
2. **Consumer** - Consume from Kafka and write to storage
3. **State Management** - Redis integration for deduplication
4. **Alerts** - Threshold-based alerting on log patterns
5. **Monitoring** - Prometheus metrics export
6. **Authentication** - API key validation
7. **Rate Limiting** - Per-tenant rate limits

---

## Files Modified/Created

| File | Purpose |
|------|---------|
| `internal/worker/worker.go` | Worker pool with batching & async processing |
| `internal/processor/processor.go` | Updated with full pipeline orchestration |
| `internal/handlers/ingest.go` | Already existed, now integrated |
| `internal/kafka/producer.go` | Already existed, now used by workers |
| `cmd/processor/main.go` | Updated with proper signal handling |
| `test/worker_test/worker_test.go` | Worker pool unit tests |
| `test/integration_test/integration_test.go` | End-to-end pipeline test |
| `PIPELINE.md` | Pipeline documentation |
| `test_pipeline.sh` | Manual testing script |

---

## Success Criteria âœ…

- âœ… Handler sends events to producer interface
- âœ… Partial success handling for batch errors
- âœ… Async workers for Kafka publishing
- âœ… Graceful shutdown with flush guarantees
- âœ… All tests passing
- âœ… End-to-end pipeline functional

**The pipeline is production-ready!** ğŸš€
